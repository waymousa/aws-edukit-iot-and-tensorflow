[
{
	"uri": "/getting-started.html",
	"title": "Getting started",
	"tags": [],
	"description": "",
	"content": "Install \u0026amp; Configure your local toolchain To complete this workshop you will need to use a Windows, Linux or Mac workstation which is configured with the toolchain you will need to deploy code to your Edukit device.\nTo set up your workstation visit AWS IoT EduKit Workshop and follow the instructions for your OS.\nSet up prerequisites In this section, you’ll download and install the AWS CLI for your host machine’s operating system, retrieve AWS Identity and Access Management (IAM) user access credentials to manage services with the AWS CLI, configure the AWS CLI, and finally test that the AWS CLI is working properly. This tutorial assumes that you have an AWS account and you have completed setting up your environment. If you already have the AWS CLI (version 1 or version 2) installed and configured on your machine, skip to the test section.\nOpen the PlatformIO CLI Terminal Window In the Getting Started tutorial, you installed and used PIO and the PIO terminal window. It is important to continue to use the PIO terminal window for all subsequent steps. The PIO terminal window pre-loads additional applications and libraries that your standard terminal/command prompt might not have.\nIf you’ve closed VS Code or don’t have the terminal viewport with the PlatformIO CLI loaded in VS Code, follow the steps below after opening VS Code:\n Click the PlatformIO logo on the VS Code activity bar (left most menu). From the Quick Access menu, under Miscellaneous, select New Terminal. The terminal viewport should load with a new terminal labeled PlatformIO CLI.  1 - Open PIO menu, 2 - Open new PIO Terminal, 3 - Verify you\u0026rsquo;re in the \u0026lsquo;PlatformIO CLI\u0026rsquo; terminal session\nDownloading and Installing the AWS CLI The AWS Command Line Interface (CLI) is a unified tool to manage your AWS services. With just one tool to download and configure, you can control multiple AWS services from the command line and automate them through scripts. To be able to configure the AWS CLI, you’ll first need to have an AWS account. Please sign in to AWS console or create an AWS account before proceeding.\n Ubuntu Linux v18.0+ (64-bit) MacOS 10.14+ Windows 10 (64-bit)  Retrieve the IAM user access credentials IAM is a web service that helps you securely control access to AWS resources. We recommend you create an administrative user first instead of using your root user account.\nTo retrieve your IAM user’s access credentials, follow the official docs.\nConfiguring the AWS CLI With the AWS CLI installed and the IAM user access credentials in hand, it’s time to configure the AWS CLI. One of the settings you’ll configure is the AWS region. It’s important to keep in mind that the region you’re currently using stays consistent—for purposes of this tutorial, we are standardizing on us-west-2. Using a different region or unknowingly changing regions can cause other challenges in subsequent steps, such as regional service availability.\nTo configure the AWS CLI on your host machine, enter the following command in the terminal viewport:\naws configure The CLI will prompt you to input four parameters. The fields should be filled out similar to below, with the corresponding access key Id and secret access key that was retrieved earlier for your IAM user:\nAWS Access Key ID [None]: EXAMPLEKEYIDEXAMPLE AWS Secret Access Key [None]: EXAMPLEtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY Default region name [None]: YOUR_REGION_HERE Default output format [None]: json Testing the AWS CLI With everything configured as described above, it is now time to test your AWS CLI to ensure it is working properly. First, you will verify the CLI is installed, and then validate the configuration.\nTo check the CLI is installed correctly, we will use the version option. A successful installation will output the AWS CLI version (if you receive errors, visit the troubleshooting guide):\naws --version Next, you will verify the AWS CLI is configured with your IAM credentials and US West (Oregon) region. The command you will run will check your MQTT broker endpoint for AWS IoT. It should return an address with the pattern xxxxxxxx-ats.iot.us-west-2.amazonaws.com. If you receive errors, visit the troubleshooting guide.\naws iot describe-endpoint --endpoint-type iot:Data-ATS "
},
{
	"uri": "/clone-the-project.html",
	"title": "Clone the project",
	"tags": [],
	"description": "",
	"content": "All of the projects and files exist in a GitHub repository, where you can also view the revision history of each file in the repository (repo). To clone the code for the tutorials, you’ll use the PIO interface:\nClick the PlatformIO logo on the VS Code activity bar (left most menu). From PlatformIO’s Quick Access menu, under Miscellaneous, select Clone Git Project. Paste https://github.com/waymousa/TFLite_Voice_Commands.git into the text field and then select the location you want to save the project in.\n1 - Open PIO menu, 2 - Clone git project, 3 - Paste repository URL.\nWhen you are asked to select a folder, pick a suitable location on your disk and click on the Select Repository Location button\nLast, you will see a prompt askign you to open a new window. Click on the Open New Window button.\n"
},
{
	"uri": "/configure-the-edukit.html",
	"title": "Configure the Edukit",
	"tags": [],
	"description": "",
	"content": "In this section we need to set up the WiFi so that your device can connect to the internet.\n Click the PlatformIO logo on the VS Code activity bar (left most menu). From the Quick Access menu, under Miscellaneous, select New Terminal. The terminal viewport should load with a new terminal labeled PlatformIO CLI. Run the following command to open the configuration menu for your device.  pio run --environment core2foraws --target menuconfig 4. Use the direction keys on your keyboard to go to Component config –\u0026gt; Amazon Web Services IoT Platform and open AWS IoT Endpoint Hostname to set the string. You can paste the address you copied moments ago into the box and hit enter to set that symbol. Next, go back to the configuration home screen by pressing the ESC key twice. Then select AWS IoT EduKit Configuration from the menu. Set your WiFi SSID and WiFi Password with your Wi-FI credentials. Once you are finished, press the s button on your keyboard to save, confirm the location of the file by pressing enter, followed by q to quit.\n"
},
{
	"uri": "/explore-the-code.html",
	"title": "Explore the code",
	"tags": [],
	"description": "",
	"content": "In addition to the core2aws environment which is the base part of the project, we have added some new components and included some code to support TensorFlow lite micro on the Edukit.\nTensorflow Micro Component In the project expoer, open the components directory. You will see the tflite folder. This contains the code for Tensorflow Lite for the ESP32 platform.\nThe tflite code is integrated to the ESP32 main method by creating a C task and main method as shown here.\n... #include \u0026quot;tflite_main.h\u0026quot; ... void app_main() { ... xTaskCreatePinnedToCore(\u0026amp;app_main_tflite, \u0026quot;tflite_main_task\u0026quot;, 1024 * 32, NULL, 8, NULL, 1); ... } The tflite_main.cc code is actually written in C++ so this section allows it tobe called as if it were a native C function\n... extern \u0026quot;C\u0026quot; void app_main_tflite(void) { xEventGroupWaitBits(wifi_event_group, CONNECTED_BIT, false, true, portMAX_DELAY); setup(); while (true) { loop(); } } And thats it, you can now run tasks that call the Tensorflow libraries.\nThe main task calls the main_functions.c which is located in the tflite folder. We will dive deep on the main_functions.c in the next section.\nMain functions The main_functions.c file contains multiple sections you will need to become familiar with. For those of you who are familiar with Arduino platform this will look very similar.\nFirstly, we include the Tensorflow libraries.\n#include \u0026quot;main_functions.h\u0026quot; #include \u0026quot;audio_provider.h\u0026quot; #include \u0026quot;command_responder.h\u0026quot; #include \u0026quot;feature_provider.h\u0026quot; #include \u0026quot;model.h\u0026quot; #include \u0026quot;recognize_commands.h\u0026quot; #include \u0026quot;tensorflow/lite/micro/micro_error_reporter.h\u0026quot; #include \u0026quot;tensorflow/lite/micro/micro_interpreter.h\u0026quot; #include \u0026quot;tensorflow/lite/micro/micro_mutable_op_resolver.h\u0026quot; #include \u0026quot;tensorflow/lite/micro/system_setup.h\u0026quot; #include \u0026quot;tensorflow/lite/schema/schema_generated.h\u0026quot; Intitialise Declare Variables We intitialise the code by setting the global namespace and declaring variables. In the code example below you can see the variables for the ErrorReporter, the model and the MicroIntepreter. These same variavbles will be used for any TensorFlow Lite Micro applications you might develop.\n// Globals, used for compatibility with Arduino-style sketches. namespace { tflite::ErrorReporter* error_reporter = nullptr; const tflite::Model* model = nullptr; tflite::MicroInterpreter* interpreter = nullptr; TfLiteTensor* model_input = nullptr; FeatureProvider* feature_provider = nullptr; RecognizeCommands* recognizer = nullptr; int32_t previous_time = 0; The next section sets up the TensorArena. You need to experiment with this a bit to get the memory sizing right because, whilst bigger is better, on a microcontroller you maty be wastign memory that is needed for other things. The size of your model largely determines this value.\n// Create an area of memory to use for input, output, and intermediate arrays. // The size of this will depend on the model you're using, and may need to be // determined by experimentation. constexpr int kTensorArenaSize = 10 * 1024; uint8_t tensor_arena[kTensorArenaSize]; int8_t feature_buffer[kFeatureElementCount]; int8_t* model_input_buffer = nullptr; } // namespace Load the model Next, we setup the environment with the model which we pull in from the model.cc file. More about that later in this section.\n// The name of this function is important for Arduino compatibility. void setup() { tflite::InitializeTarget(); // Set up logging. Google style is to avoid globals or statics because of // lifetime uncertainty, but since this has a trivial destructor it's okay. // NOLINTNEXTLINE(runtime-global-variables) static tflite::MicroErrorReporter micro_error_reporter; error_reporter = \u0026amp;micro_error_reporter; // Map the model into a usable data structure. This doesn't involve any // copying or parsing, it's a very lightweight operation. model = tflite::GetModel(g_model); if (model-\u0026gt;version() != TFLITE_SCHEMA_VERSION) { TF_LITE_REPORT_ERROR(error_reporter, \u0026quot;Model provided is schema version %d not equal \u0026quot; \u0026quot;to supported version %d.\u0026quot;, model-\u0026gt;version(), TFLITE_SCHEMA_VERSION); return; } Resolve Operators To make sure our model runs effieicntly, we only need to import the Tensorflow operations that our model uses. This reduces the memory footprint. Its important to understand the operations your model needs rather than just using the all_operations code. See the resources section for more on how to determine this usign the python tools for Tensorflow. We are using Reshape, DepthwiseConv2D, FullyConnectioned and Softmax operations in our model so thats what we set up here.\n // // tflite::AllOpsResolver resolver; // NOLINTNEXTLINE(runtime-global-variables) static tflite::MicroMutableOpResolver\u0026lt;4\u0026gt; micro_op_resolver(error_reporter); if (micro_op_resolver.AddDepthwiseConv2D() != kTfLiteOk) { return; } if (micro_op_resolver.AddFullyConnected() != kTfLiteOk) { return; } if (micro_op_resolver.AddSoftmax() != kTfLiteOk) { return; } if (micro_op_resolver.AddReshape() != kTfLiteOk) { return; } Intitialize Intepreter Next, we create an interpreter to run the model with, by passing in the tensor_arena, operations resolver, the model and the error reporter.\n // Build an interpreter to run the model with. static tflite::MicroInterpreter static_interpreter( model, micro_op_resolver, tensor_arena, kTensorArenaSize, error_reporter); interpreter = \u0026amp;static_interpreter; Allocate the arena Next we will allocate an area of memory that the tensors are going to run out of.\n // Allocate memory from the tensor_arena for the model's tensors. TfLiteStatus allocate_status = interpreter-\u0026gt;AllocateTensors(); if (allocate_status != kTfLiteOk) { TF_LITE_REPORT_ERROR(error_reporter, \u0026quot;AllocateTensors() failed\u0026quot;); return; } Define Model Inputs And then we set up some input buffers for the Tensors so we can feed the microphone data to the model. You mad the mode_input to the intepreter input\n // Get information about the memory area to use for the model's input. model_input = interpreter-\u0026gt;input(0); if ((model_input-\u0026gt;dims-\u0026gt;size != 2) || (model_input-\u0026gt;dims-\u0026gt;data[0] != 1) || (model_input-\u0026gt;dims-\u0026gt;data[1] != (kFeatureSliceCount * kFeatureSliceSize)) || (model_input-\u0026gt;type != kTfLiteInt8)) { TF_LITE_REPORT_ERROR(error_reporter, \u0026quot;Bad input tensor parameters in model\u0026quot;); return; } model_input_buffer = model_input-\u0026gt;data.int8; Now that Tensorflow is set up, we can configure th eother applications we will need. FeatureProvider is an app that converst the audio input to Spectograms. RecognizeCommands is the code we use uise to do something about the commands we recognise.\n // Prepare to access the audio spectrograms from a microphone or other source // that will provide the inputs to the neural network. // NOLINTNEXTLINE(runtime-global-variables) static FeatureProvider static_feature_provider(kFeatureElementCount, feature_buffer); feature_provider = \u0026amp;static_feature_provider; static RecognizeCommands static_recognizer(error_reporter); recognizer = \u0026amp;static_recognizer; previous_time = 0; } Setup the main loop The main loop will run 5 operations continously:\n Feature Extractor: feature_provider-\u0026gt;PopulateFeatureData The Audio provider: GetAudioSamples (audio_provider.cc) is called in the feature provider. it is intialized as a task and runs continuosly. Intepreter: interpreter-\u0026gt;Invoke Command Recognizer: recognizer-\u0026gt;ProcessLatestResults Command responder: RespondToCommand  // The name of this function is important for Arduino compatibility. void loop() { // Fetch the spectrogram for the current time. const int32_t current_time = LatestAudioTimestamp(); int how_many_new_slices = 0; TfLiteStatus feature_status = feature_provider-\u0026gt;PopulateFeatureData( error_reporter, previous_time, current_time, \u0026amp;how_many_new_slices); if (feature_status != kTfLiteOk) { TF_LITE_REPORT_ERROR(error_reporter, \u0026quot;Feature generation failed\u0026quot;); return; } previous_time = current_time; // If no new audio samples have been received since last time, don't bother // running the network model. if (how_many_new_slices == 0) { return; } // Copy feature buffer to input tensor for (int i = 0; i \u0026lt; kFeatureElementCount; i++) { model_input_buffer[i] = feature_buffer[i]; } // Run the model on the spectrogram input and make sure it succeeds. TfLiteStatus invoke_status = interpreter-\u0026gt;Invoke(); if (invoke_status != kTfLiteOk) { TF_LITE_REPORT_ERROR(error_reporter, \u0026quot;Invoke failed\u0026quot;); return; } // Obtain a pointer to the output tensor TfLiteTensor* output = interpreter-\u0026gt;output(0); // Determine whether a command was recognized based on the output of inference const char* found_command = nullptr; uint8_t score = 0; bool is_new_command = false; TfLiteStatus process_status = recognizer-\u0026gt;ProcessLatestResults( output, current_time, \u0026amp;found_command, \u0026amp;score, \u0026amp;is_new_command); if (process_status != kTfLiteOk) { TF_LITE_REPORT_ERROR(error_reporter, \u0026quot;RecognizeCommands::ProcessLatestResults() failed\u0026quot;); return; } // Do something based on the recognized command. The default implementation // just prints to the error console, but you should replace this with your // own function for a real application. RespondToCommand(error_reporter, current_time, found_command, score, is_new_command); } Keywork Detection Model and code I the project explorer, open the includes directory.\nmodel.cc This code contains the tensorflow model that was trained in the Cloud. The model itself is just a hexidecimal dump of the model that was trained usign Tensorflow. This model is stored a variable g_model[] in the C source file. The model itself is a TinyConv or Convolution Neural Network model that has been trained using 8bit quantised audio data.\n#include \u0026quot;model.h\u0026quot; alignas(8) const unsigned char g_model[] = { 0x20, 0x00, 0x00, 0x00, 0x54, 0x46, 0x4c, 0x33, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x12, 0x00, 0x1c, 0x00, 0x04, 0x00, 0x08, 0x00, 0x0c, 0x00, 0x10, 0x00, 0x14, 0x00, 0x00, 0x00, 0x18, 0x00, 0x12, 0x00, 0x00, 0x00, ... 0x03, 0x00, 0x00, 0x00 }; unsigned int micro_speech_tflite_len = 18712; Note the len variable at the bottom of the file. You will need to update this when you train your own models with the size of the C variable so that the microcopntroller can allocate the right amount of memory for it. Yes! you need to worry about memory usage now because your dealing with a microcontroller and space is finite :-)\nIn here you will see the supporting code from the Tensorflow project that allows the Edukit enable the microphone, write microphone data to a ringbuffer, read the buffer and convert the audio to a spectogram, run inferences using the Tensroflow model, confirm if a command was detected and respond to that command.\nTying it all together Now that you have seen the code layout in the project, you can tie it all together by running the tflite task in the main.c for the application.\n"
},
{
	"uri": "/build-upload-and-monitor.html",
	"title": "Build, upload and monitor",
	"tags": [],
	"description": "",
	"content": "In this section we build and deploy the code to the Edukit, then monitor it to see if the model is detecting keywords..\n Click the PlatformIO logo on the VS Code activity bar (left most menu). From the Quick Access menu, under Miscellaneous, select New Terminal. The terminal viewport should load with a new terminal labeled PlatformIO CLI. Run the following command to build the firmware for your device.  pio run --environment core2foraws With the build sucessful, run the following command to upload the copmpiled code to your device over USB.  pio run --environment core2foraws --target upload After the device reboots you should see the display light up with a message saying Tensorflow for a few minutes.\nLastly, monitor the serial output from the device on your host machine.  pio run --environment core2foraws --target monitor You should now see the serial output showing a continous stream of recordings being exampined for keywords. Try it out by speakling clearly into the microphone on the Edukit and saying \u0026ldquo;yes\u0026rdquo; or \u0026ldquo;no\u0026rdquo;.\n"
},
{
	"uri": "/customize-your-code.html",
	"title": "Customize the code",
	"tags": [],
	"description": "",
	"content": "In this section we will chaneg the command responder code so that it causes the Edukit LED\u0026rsquo;s to light up. This is called actuating the Edukit. You can use the same approach to actuate any response the EduKit can handle, such as posting an MQTT message, activatying a motor or maching the state of machinery.\n"
},
{
	"uri": "/next-steps.html",
	"title": "Next Steps",
	"tags": [],
	"description": "",
	"content": "Now that you have had some hands on experience with deploying and running models on the edge you can refine the project to do additional things.\nImprove model performance The model has been created with a very basic data set so it can\u0026rsquo;t know for certain what it heard all the time because it doesn\u0026rsquo;t know your voice. To improve the performance of the model you need to train your model with new data so it recognises you.\nUpdate model over the air As your model gets better over time you can create a pipeline to update the model and deploy the code using the OTA feature.\nDeploy a cascade architecture Now that you have a model that is updating at the edge, you can start summarising inference data and sending it to the cloud for further processing. MCU devices can do some inference, but for more advanced utterances the data needs to be sent to cloud for more powerful processing. You can use MQTT to help you do this.\nResources and further reading Here are links to additional resources you can use.\n"
},
{
	"uri": "/",
	"title": "AWS Edukit IoT and Tensorflow",
	"tags": [],
	"description": "",
	"content": "This workshop module will take you through the steps required to deploy and test a Tensorflow model on the AWS EduKit IoT device.\nAt the end of the module your Edukit device will be able to detect and respond to keywords.\n"
},
{
	"uri": "/categories.html",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags.html",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]